{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains our first model for our emotions classifier. It is a fractal neural network (similar to a residual neural network, but more modern). The code is sourced from here: https://github.com/snf/keras-fractalnet/blob/master/. Paper is located here: http://www.fractal.org/Life-Science-Technology/Publications/Fractal-Neural-Networks.htm\n",
    "                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This first segment contains helper functions/ classes (fractal.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.layers import (\n",
    "    Input,\n",
    "    BatchNormalization,\n",
    "    Activation, Dense, Dropout,\n",
    "    Conv2D, MaxPooling2D, ZeroPadding2D\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.engine import Layer\n",
    "#from keras.utils.visualize_util import plot\n",
    "from keras import backend as K\n",
    "\n",
    "if K._BACKEND == 'theano':\n",
    "    from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams\n",
    "if K._BACKEND == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "\n",
    "def theano_multinomial(n, pvals, seed):\n",
    "    rng = RandomStreams(seed)\n",
    "    return rng.multinomial(n=n, pvals=pvals, dtype='float32')\n",
    "\n",
    "def tensorflow_categorical(count, seed):\n",
    "    assert count > 0\n",
    "    arr = [1.] + [.0 for _ in range(count-1)]\n",
    "    return tf.random_shuffle(arr, seed)\n",
    "\n",
    "# Returns a random array [x0, x1, ...xn] where one is 1 and the others\n",
    "# are 0. Ex: [0, 0, 1, 0].\n",
    "def rand_one_in_array(count, seed=None):\n",
    "    if seed is None:\n",
    "        seed = np.random.randint(1, 10e6)\n",
    "    if K._BACKEND == 'theano':\n",
    "        pvals = np.array([[1. / count for _ in range(count)]], dtype='float32')\n",
    "        return theano_multinomial(n=1, pvals=pvals, seed=seed)[0]\n",
    "    elif K._BACKEND == 'tensorflow':\n",
    "        return tensorflow_categorical(count=count, seed=seed)\n",
    "    else:\n",
    "        raise Exception('Backend: {} not implemented'.format(K._BACKEND))\n",
    "\n",
    "class JoinLayer(Layer):\n",
    "    '''\n",
    "    This layer will behave as Merge(mode='ave') during testing but\n",
    "    during training it will randomly select between using local or\n",
    "    global droppath and apply the average of the paths alive after\n",
    "    aplying the drops.\n",
    "    - Global: use the random shared tensor to select the paths.\n",
    "    - Local: sample a random tensor to select the paths.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, drop_p, is_global, global_path, force_path, **kwargs):\n",
    "        #print \"init\"\n",
    "        self.p = 1. - drop_p\n",
    "        self.is_global = is_global\n",
    "        self.global_path = global_path\n",
    "        self.uses_learning_phase = True\n",
    "        self.force_path = force_path\n",
    "        super(JoinLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(\"build\")\n",
    "        self.average_shape = list(input_shape[0])[1:]\n",
    "\n",
    "    def _random_arr(self, count, p):\n",
    "        return K.random_binomial((count,), p=p)\n",
    "\n",
    "    def _arr_with_one(self, count):\n",
    "        return rand_one_in_array(count=count)\n",
    "\n",
    "    def _gen_local_drops(self, count, p):\n",
    "        # Create a local droppath with at least one path\n",
    "        arr = self._random_arr(count, p)\n",
    "        drops = K.switch(\n",
    "            K.any(arr),\n",
    "            arr,\n",
    "            self._arr_with_one(count)\n",
    "        )\n",
    "        return drops\n",
    "\n",
    "    def _gen_global_path(self, count):\n",
    "        return self.global_path[:count]\n",
    "\n",
    "    def _drop_path(self, inputs):\n",
    "        count = len(inputs)\n",
    "        drops = K.switch(\n",
    "            self.is_global,\n",
    "            self._gen_global_path(count),\n",
    "            self._gen_local_drops(count, self.p)\n",
    "        )\n",
    "        ave = K.zeros(shape=self.average_shape)\n",
    "        for i in range(0, count):\n",
    "            ave = ave + inputs[i] * drops[i]\n",
    "        sum = K.sum(drops)\n",
    "        # Check that the sum is not 0 (global droppath can make it\n",
    "        # 0) to avoid divByZero\n",
    "        ave = K.switch(\n",
    "            K.not_equal(sum, 0.),\n",
    "            ave/sum,\n",
    "            ave)\n",
    "        return ave\n",
    "\n",
    "    def _ave(self, inputs):\n",
    "        ave = inputs[0]\n",
    "        for input in inputs[1:]:\n",
    "            ave = ave + input\n",
    "        ave /= len(inputs)\n",
    "        return ave\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        #print(\"call\")\n",
    "        if self.force_path:\n",
    "            output = self._drop_path(inputs)\n",
    "        else:\n",
    "            output = K.in_train_phase(self._drop_path(inputs), self._ave(inputs))\n",
    "        return output\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        #print(\"get_output_shape_for\", input_shape)\n",
    "        return input_shape[0]\n",
    "\n",
    "class JoinLayerGen:\n",
    "    '''\n",
    "    JoinLayerGen will initialize seeds for both global droppath\n",
    "    switch and global droppout path.\n",
    "    These seeds will be used to create the random tensors that the\n",
    "    children layers will use to know if they must use global droppout\n",
    "    and which path to take in case it is.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, width, global_p=0.5, deepest=False):\n",
    "        self.global_p = global_p\n",
    "        self.width = width\n",
    "        self.switch_seed = np.random.randint(1, 10e6)\n",
    "        self.path_seed = np.random.randint(1, 10e6)\n",
    "        self.deepest = deepest\n",
    "        if deepest:\n",
    "            self.is_global = K.variable(1.)\n",
    "            self.path_array = K.variable([1.] + [.0 for _ in range(width-1)])\n",
    "        else:\n",
    "            self.is_global = self._build_global_switch()\n",
    "            self.path_array = self._build_global_path_arr()\n",
    "\n",
    "    def _build_global_path_arr(self):\n",
    "        # The path the block will take when using global droppath\n",
    "        return rand_one_in_array(seed=self.path_seed, count=self.width)\n",
    "\n",
    "    def _build_global_switch(self):\n",
    "        # A randomly sampled tensor that will signal if the batch\n",
    "        # should use global or local droppath\n",
    "        return K.equal(K.random_binomial((), p=self.global_p, seed=self.switch_seed), 1.)\n",
    "\n",
    "    def get_join_layer(self, drop_p):\n",
    "        global_switch = self.is_global\n",
    "        global_path = self.path_array\n",
    "        return JoinLayer(drop_p=drop_p, is_global=global_switch, global_path=global_path, force_path=self.deepest)\n",
    "\n",
    "def fractal_conv(filter, nb_row, nb_col, dropout=None):\n",
    "    def f(prev):\n",
    "        conv = prev\n",
    "        conv = Conv2D(filter, nb_row=nb_col, nb_col=nb_col, kernel_initializer='he_normal', padding='same')(conv)\n",
    "        if dropout:\n",
    "            conv = Dropout(dropout)(conv)\n",
    "        conv = BatchNormalization(mode=0, axis=1 if K._BACKEND == 'theano' else -1)(conv)\n",
    "        conv = Activation('relu')(conv)\n",
    "        return conv\n",
    "    return f\n",
    "\n",
    "# XXX_ It's not clear when to apply Dropout, the paper cited\n",
    "# (arXiv:1511.07289) uses it in the last layer of each stack but in\n",
    "# the code gustav published it is in each convolution block so I'm\n",
    "# copying it.\n",
    "def fractal_block(join_gen, c, filter, nb_col, nb_row, drop_p, dropout=None):\n",
    "    def f(z):\n",
    "        columns = [[z] for _ in range(c)]\n",
    "        last_row = 2**(c-1) - 1\n",
    "        for row in range(2**(c-1)):\n",
    "            t_row = []\n",
    "            for col in range(c):\n",
    "                prop = 2**(col)\n",
    "                # Add blocks\n",
    "                if (row+1) % prop == 0:\n",
    "                    t_col = columns[col]\n",
    "                    t_col.append(fractal_conv(filter=filter,\n",
    "                                              nb_col=nb_col,\n",
    "                                              nb_row=nb_row,\n",
    "                                              dropout=dropout)(t_col[-1]))\n",
    "                    t_row.append(col)\n",
    "            # Merge (if needed)\n",
    "            if len(t_row) > 1:\n",
    "                merging = [columns[x][-1] for x in t_row]\n",
    "                merged  = join_gen.get_join_layer(drop_p=drop_p)(merging)\n",
    "                for i in t_row:\n",
    "                    columns[i].append(merged)\n",
    "        return columns[0][-1]\n",
    "    return f\n",
    "\n",
    "def fractal_net(b, c, conv, drop_path, global_p=0.5, dropout=None, deepest=False):\n",
    "    '''\n",
    "    Return a function that builds the Fractal part of the network\n",
    "    respecting keras functional model.\n",
    "    When deepest is set, we build the entire network but set droppath\n",
    "    to global and the Join masks to [1., 0... 0.] so only the deepest\n",
    "    column is always taken.\n",
    "    We don't add the softmax layer here nor build the model.\n",
    "    '''\n",
    "    def f(z):\n",
    "        output = z\n",
    "        # Initialize a JoinLayerGen that will be used to derive the\n",
    "        # JoinLayers that share the same global droppath\n",
    "        join_gen = JoinLayerGen(width=c, global_p=global_p, deepest=deepest)\n",
    "        for i in range(b):\n",
    "            (filter, nb_col, nb_row) = conv[i]\n",
    "            dropout_i = dropout[i] if dropout else None\n",
    "            output = fractal_block(join_gen=join_gen,\n",
    "                                   c=c, filter=filter,\n",
    "                                   nb_col=nb_col,\n",
    "                                   nb_row=nb_row,\n",
    "                                   drop_p=drop_path,\n",
    "                                   dropout=dropout_i)(output)\n",
    "            output = MaxPooling2D(pool_size=(2,2), strides=(2,2))(output)\n",
    "        return output\n",
    "    return f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next segment contains handwritten code to load train/dev/test data into arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished reading dev set\n",
      "Finished reading test set\n",
      "Finished reading train set\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "IMAGE_HEIGHT = IMAGE_WIDTH = 32\n",
    "NUM_CHANNELS = 3\n",
    "\n",
    "def path_to(dataset, imagenum):\n",
    "    return \"C:\\\\Users\\\\noahj\\\\cs230\\\\CS230 Project\\\\\" + dataset + \"_img\\\\img{}.jpg\".format(imagenum)\n",
    "\n",
    "def load_data():\n",
    "    print(\"Starting\")\n",
    "    with open('labels.json') as f_labels:\n",
    "        labels = json.load(f_labels)\n",
    "        y_train = np.expand_dims(np.asarray(labels[\"trainY\"]), axis = 1)\n",
    "        y_dev = np.expand_dims(np.asarray(labels[\"devY\"]), axis = 1)\n",
    "        y_test = np.expand_dims(np.asarray(labels[\"testY\"]), axis = 1)\n",
    "        # X Dev\n",
    "        X_dev = np.zeros((len(labels[\"devX\"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))\n",
    "        for imagenum in range(len(labels[\"devX\"])):\n",
    "            im_arr = cv2.imread(path_to(\"dev\", imagenum))\n",
    "            X_dev[imagenum] = im_arr\n",
    "        print(\"Finished reading dev set\")\n",
    "        # X Test\n",
    "        X_test = np.zeros((len(labels[\"testX\"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))\n",
    "        for imagenum in range(len(labels[\"testX\"])):\n",
    "            im_arr = cv2.imread(path_to(\"test\", imagenum))\n",
    "            X_test[imagenum] = im_arr\n",
    "        print(\"Finished reading test set\")\n",
    "        # X Train\n",
    "        X_train = np.zeros((len(labels[\"trainX\"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))\n",
    "        for imagenum in range(len(labels[\"trainX\"])):\n",
    "            im_arr = cv2.imread(path_to(\"train\", imagenum))\n",
    "            X_train[imagenum] = im_arr\n",
    "        print(\"Finished reading train set\")\n",
    "    return (X_train, y_train), (X_dev, y_dev), (X_test, y_test)\n",
    "\n",
    "(X_train, y_train), (X_dev, y_dev), (X_test, y_test) = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we are ready to train the neural network with out data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next segment contains the code that actually runs (cifar10_fractal.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:162: UserWarning: Update your `BatchNormalization` call to the Keras 2 API: `BatchNormalization(axis=-1)`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(256, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (3, 3), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:159: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(512, (2, 2), kernel_initializer=\"he_normal\", padding=\"same\")`\n",
      "C:\\Users\\noahj\\Anaconda3\\envs\\cs\\lib\\site-packages\\ipykernel_launcher.py:64: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"ac...)`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network\n",
      "Train on 67938 samples, validate on 8493 samples\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-3a8de9d3bc33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-3a8de9d3bc33>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;31m#        classify_image(net, weights, image)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m         \u001b[0mtrain_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-3a8de9d3bc33>\u001b[0m in \u001b[0;36mtrain_network\u001b[1;34m(net)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNB_EPOCHS\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m         \u001b[1;31m#callbacks=[learn, snapshot]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 85\u001b[1;33m         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msnapshot\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     86\u001b[0m     )\n\u001b[0;32m     87\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1040\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 199\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    200\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    201\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2696\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2697\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_make_callable_from_options'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2698\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2699\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    204\u001b[0m                     \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m                     \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;31m# hack for list_devices() function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m     \u001b[1;31m# list_devices() function is not available under tensorflow r1.3.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    885\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 887\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    888\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1108\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1110\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1111\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1284\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1286\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1288\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1290\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1291\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1292\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1293\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1277\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1279\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\cs\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1367\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1368\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1369\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from keras.callbacks import (\n",
    "    LearningRateScheduler,\n",
    "    ModelCheckpoint\n",
    ")\n",
    "from keras.datasets import cifar10\n",
    "from keras.layers import (\n",
    "    Activation,\n",
    "    Input,\n",
    "    Dense,\n",
    "    Flatten\n",
    ")\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, RMSprop, Adam, Nadam\n",
    "#from keras.utils.visualize_util import plot\n",
    "from keras.utils import np_utils\n",
    "from keras import backend as K\n",
    "\n",
    "\n",
    "NB_CLASSES = 8 \n",
    "NB_EPOCHS = 10 #Original value is 400\n",
    "LEARN_START = 0.02\n",
    "BATCH_SIZE = 100\n",
    "MOMENTUM = 0.9\n",
    "IMG_SIZE = 32\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, NB_CLASSES)\n",
    "Y_dev = np_utils.to_categorical(y_dev, NB_CLASSES)\n",
    "Y_test = np_utils.to_categorical(y_test, NB_CLASSES)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_dev = X_dev.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "\n",
    "X_train /= 255\n",
    "X_dev /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Drop by 10 when we halve the number of remaining epochs (200, 300, 350, 375)\n",
    "def learning_rate(epoch):\n",
    "    if epoch < 200:\n",
    "        return 0.02\n",
    "    if epoch < 300:\n",
    "        return 0.002\n",
    "    if epoch < 350:\n",
    "        return 0.0002\n",
    "    if epoch < 375:\n",
    "        return 0.00002\n",
    "    return 0.000002\n",
    "\n",
    "def build_network(deepest=False):\n",
    "    dropout = [0., 0.1, 0.2, 0.3, 0.4]\n",
    "    conv = [(64, 3, 3), (128, 3, 3), (256, 3, 3), (512, 3, 3), (512, 2, 2)]\n",
    "    input= Input(shape=(3, IMG_SIZE, IMG_SIZE) if K._BACKEND == 'theano' else (IMG_SIZE, IMG_SIZE,3))\n",
    "    output = fractal_net(\n",
    "        c=3, b=5, conv=conv,\n",
    "        drop_path=0.15, dropout=dropout,\n",
    "        deepest=deepest)(input)\n",
    "    output = Flatten()(output)\n",
    "    output = Dense(NB_CLASSES, kernel_initializer=\"he_normal\")(output)\n",
    "    output = Activation('softmax')(output)\n",
    "    model = Model(input=input, output=output)\n",
    "    #optimizer = SGD(lr=LEARN_START, momentum=MOMENTUM)\n",
    "    #optimizer = SGD(lr=LEARN_START, momentum=MOMENTUM, nesterov=True)\n",
    "    optimizer = Adam()\n",
    "    #optimizer = Nadam()\n",
    "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    #plot(model, to_file='model.png', show_shapes=True)\n",
    "    return model\n",
    "\n",
    "def train_network(net):\n",
    "    print(\"Training network\")\n",
    "    snapshot = ModelCheckpoint(\n",
    "        filepath=\"snapshots/weights.{epoch:04d}-{val_loss:.4f}.h5\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=False)\n",
    "    learn = LearningRateScheduler(learning_rate)\n",
    "    \n",
    "    net.fit(\n",
    "        x=X_train, y=Y_train, batch_size=BATCH_SIZE,\n",
    "        epochs=NB_EPOCHS, valiadation_data=(X_dev, Y_dev),\n",
    "        #callbacks=[learn, snapshot]\n",
    "        callbacks=[snapshot]\n",
    "    )\n",
    "\n",
    "def test_network(net, weights):\n",
    "    print(\"Loading weights from '{}' and testing\".format(weights))\n",
    "    net.load_weights(weights)\n",
    "    ret = net.evaluate(x=X_test, y=Y_test, batch_size=BATCH_SIZE)\n",
    "    print('Test:', ret)\n",
    "\n",
    "def classify_image(net, weights, img):\n",
    "    print(\"Loading weights from '{}' and testing\".format(weights))\n",
    "    net.load_weights(weights)\n",
    "    ret = net.predict(img)\n",
    "    print('Predictions: ', ret)\n",
    "    \n",
    "def main():\n",
    "    # Since argparsers don't work in jupyter notebooks, enter all args manually here. \n",
    "    args = {\"deepest\": False, \"load\": None, \"summary\": False}\n",
    "    classify = False;\n",
    "    net = build_network(deepest=args[\"deepest\"])\n",
    "    if args[\"load\"]:\n",
    "        weights_filepath = \"snapshots/weights.{epoch:04d}-{val_loss:.4f}.h5\" #Fill this in with the location of the file containting the weights\n",
    "        weights = weights_filepath\n",
    "        test_network(net, weights)\n",
    "    elif args[\"summary\"]:\n",
    "        net.summary()\n",
    "#    elif classify:\n",
    "#        image = \"C:\\\\Users\\\\noahj\\\\cs230\\\\CS230 Project\\\\train_img\\\\img0.jpg\"\n",
    "#        image = np.expand_dims(cv2.imread(image, imagenum), axis=0)\n",
    "#        classify_image(net, weights, image)\n",
    "    else:\n",
    "        train_network(net)\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
