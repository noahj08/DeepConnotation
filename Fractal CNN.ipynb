This notebook contains our first model for our emotions classifier. It is a fractal neural network (similar to a residual neural network, but more modern). The code is sourced from here: https://github.com/snf/keras-fractalnet/blob/master/. Paper is located here: http://www.fractal.org/Life-Science-Technology/Publications/Fractal-Neural-Networks.htm

This first segment contains helper functions/ classes (fractal.py)


1
import numpy as np
2
from keras.layers import (
3
    Input,
4
    BatchNormalization,
5
    Activation, Dense, Dropout,
6
    Conv2D, MaxPooling2D, ZeroPadding2D
7
)
8
from keras.models import Model
9
from keras.engine import Layer
10
#from keras.utils.visualize_util import plot
11
from keras import backend as K
12
​
13
if K._BACKEND == 'theano':
14
    from theano.sandbox.rng_mrg import MRG_RandomStreams as RandomStreams
15
if K._BACKEND == 'tensorflow':
16
    import tensorflow as tf
17
​
18
def theano_multinomial(n, pvals, seed):
19
    rng = RandomStreams(seed)
20
    return rng.multinomial(n=n, pvals=pvals, dtype='float32')
21
​
22
def tensorflow_categorical(count, seed):
23
    assert count > 0
24
    arr = [1.] + [.0 for _ in range(count-1)]
25
    return tf.random_shuffle(arr, seed)
26
​
27
# Returns a random array [x0, x1, ...xn] where one is 1 and the others
28
# are 0. Ex: [0, 0, 1, 0].
29
def rand_one_in_array(count, seed=None):
30
    if seed is None:
31
        seed = np.random.randint(1, 10e6)
32
    if K._BACKEND == 'theano':
33
        pvals = np.array([[1. / count for _ in range(count)]], dtype='float32')
34
        return theano_multinomial(n=1, pvals=pvals, seed=seed)[0]
35
    elif K._BACKEND == 'tensorflow':
36
        return tensorflow_categorical(count=count, seed=seed)
37
    else:
38
        raise Exception('Backend: {} not implemented'.format(K._BACKEND))
39
​
40
class JoinLayer(Layer):
41
    '''
42
    This layer will behave as Merge(mode='ave') during testing but
43
    during training it will randomly select between using local or
44
    global droppath and apply the average of the paths alive after
45
    aplying the drops.
46
    - Global: use the random shared tensor to select the paths.
47
    - Local: sample a random tensor to select the paths.
48
    '''
49
​
50
    def __init__(self, drop_p, is_global, global_path, force_path, **kwargs):
51
        #print "init"
52
        self.p = 1. - drop_p
53
        self.is_global = is_global
54
        self.global_path = global_path
55
        self.uses_learning_phase = True
56
        self.force_path = force_path
57
        super(JoinLayer, self).__init__(**kwargs)
58
​
59
    def build(self, input_shape):
60
        #print("build")
61
        self.average_shape = list(input_shape[0])[1:]
62
​
63
    def _random_arr(self, count, p):
64
        return K.random_binomial((count,), p=p)
65
​
66
    def _arr_with_one(self, count):
67
        return rand_one_in_array(count=count)
68
​
69
    def _gen_local_drops(self, count, p):
70
        # Create a local droppath with at least one path
71
        arr = self._random_arr(count, p)
72
        drops = K.switch(
73
            K.any(arr),
74
            arr,
75
            self._arr_with_one(count)
76
        )
77
        return drops
78
​
79
    def _gen_global_path(self, count):
80
        return self.global_path[:count]
81
​
82
    def _drop_path(self, inputs):
83
        count = len(inputs)
84
        drops = K.switch(
85
            self.is_global,
86
            self._gen_global_path(count),
87
            self._gen_local_drops(count, self.p)
88
        )
89
        ave = K.zeros(shape=self.average_shape)
90
        for i in range(0, count):
91
            ave += inputs[i] * drops[i]
92
        sum = K.sum(drops)
93
        # Check that the sum is not 0 (global droppath can make it
94
        # 0) to avoid divByZero
95
        ave = K.switch(
96
            K.not_equal(sum, 0.),
97
            ave/sum,
98
            ave)
99
        return ave
100
​
101
    def _ave(self, inputs):
102
        ave = inputs[0]
103
        for input in inputs[1:]:
104
            ave += input
105
        ave /= len(inputs)
106
        return ave
107
​
108
    def call(self, inputs, mask=None):
109
        #print("call")
110
        if self.force_path:
111
            output = self._drop_path(inputs)
112
        else:
113
            output = K.in_train_phase(self._drop_path(inputs), self._ave(inputs))
114
        return output
115
​
116
    def get_output_shape_for(self, input_shape):
117
        #print("get_output_shape_for", input_shape)
118
        return input_shape[0]
119
​
120
class JoinLayerGen:
121
    '''
122
    JoinLayerGen will initialize seeds for both global droppath
123
    switch and global droppout path.
124
    These seeds will be used to create the random tensors that the
125
    children layers will use to know if they must use global droppout
126
    and which path to take in case it is.
127
    '''
128
​
129
    def __init__(self, width, global_p=0.5, deepest=False):
130
        self.global_p = global_p
131
        self.width = width
132
        self.switch_seed = np.random.randint(1, 10e6)
133
        self.path_seed = np.random.randint(1, 10e6)
134
        self.deepest = deepest
135
        if deepest:
136
            self.is_global = K.variable(1.)
137
            self.path_array = K.variable([1.] + [.0 for _ in range(width-1)])
138
        else:
139
            self.is_global = self._build_global_switch()
140
            self.path_array = self._build_global_path_arr()
141
​
142
    def _build_global_path_arr(self):
143
        # The path the block will take when using global droppath
144
        return rand_one_in_array(seed=self.path_seed, count=self.width)
145
​
146
    def _build_global_switch(self):
147
        # A randomly sampled tensor that will signal if the batch
148
        # should use global or local droppath
149
        return K.equal(K.random_binomial((), p=self.global_p, seed=self.switch_seed), 1.)
150
​
151
    def get_join_layer(self, drop_p):
152
        global_switch = self.is_global
153
        global_path = self.path_array
154
        return JoinLayer(drop_p=drop_p, is_global=global_switch, global_path=global_path, force_path=self.deepest)
155
​
156
def fractal_conv(filter, nb_row, nb_col, dropout=None):
157
    def f(prev):
158
        conv = prev
159
        conv = Conv2D(filter, nb_row=nb_col, nb_col=nb_col, kernel_initializer='he_normal', padding='same')(conv)
160
        if dropout:
161
            conv = Dropout(dropout)(conv)
162
        conv = BatchNormalization(mode=0, axis=1 if K._BACKEND == 'theano' else -1)(conv)
163
        conv = Activation('relu')(conv)
164
        return conv
165
    return f
166
​
167
# XXX_ It's not clear when to apply Dropout, the paper cited
168
# (arXiv:1511.07289) uses it in the last layer of each stack but in
169
# the code gustav published it is in each convolution block so I'm
170
# copying it.
171
def fractal_block(join_gen, c, filter, nb_col, nb_row, drop_p, dropout=None):
172
    def f(z):
173
        columns = [[z] for _ in range(c)]
174
        last_row = 2**(c-1) - 1
175
        for row in range(2**(c-1)):
176
            t_row = []
177
            for col in range(c):
178
                prop = 2**(col)
179
                # Add blocks
180
                if (row+1) % prop == 0:
181
                    t_col = columns[col]
182
                    t_col.append(fractal_conv(filter=filter,
183
                                              nb_col=nb_col,
184
                                              nb_row=nb_row,
185
                                              dropout=dropout)(t_col[-1]))
186
                    t_row.append(col)
187
            # Merge (if needed)
188
            if len(t_row) > 1:
189
                merging = [columns[x][-1] for x in t_row]
190
                merged  = join_gen.get_join_layer(drop_p=drop_p)(merging)
191
                for i in t_row:
192
                    columns[i].append(merged)
193
        return columns[0][-1]
194
    return f
195
​
196
def fractal_net(b, c, conv, drop_path, global_p=0.5, dropout=None, deepest=False):
197
    '''
198
    Return a function that builds the Fractal part of the network
199
    respecting keras functional model.
200
    When deepest is set, we build the entire network but set droppath
201
    to global and the Join masks to [1., 0... 0.] so only the deepest
202
    column is always taken.
203
    We don't add the softmax layer here nor build the model.
204
    '''
205
    def f(z):
206
        output = z
207
        # Initialize a JoinLayerGen that will be used to derive the
208
        # JoinLayers that share the same global droppath
209
        join_gen = JoinLayerGen(width=c, global_p=global_p, deepest=deepest)
210
        for i in range(b):
211
            (filter, nb_col, nb_row) = conv[i]
212
            dropout_i = dropout[i] if dropout else None
213
            output = fractal_block(join_gen=join_gen,
214
                                   c=c, filter=filter,
215
                                   nb_col=nb_col,
216
                                   nb_row=nb_row,
217
                                   drop_p=drop_path,
218
                                   dropout=dropout_i)(output)
219
            output = MaxPooling2D(pool_size=(2,2), strides=(2,2))(output)
220
        return output
221
    return f
The next segment contains handwritten code to load train/dev/test data into arrays


1
import cv2
2
import json
3
import numpy as np
4
import matplotlib.pyplot as plt
5
​
6
IMAGE_HEIGHT = IMAGE_WIDTH = 32
7
NUM_CHANNELS = 3
8
​
9
def path_to(dataset, imagenum):
10
    return "C:\\Users\\noahj\\cs230\\CS230 Project\\" + dataset + "_img\\img{}.jpg".format(imagenum)
11
​
12
def load_data():
13
    with open('labels.json') as f_labels:
14
        labels = json.load(f_labels)
15
        y_train = np.expand_dims(np.asarray(labels["trainY"]), axis = 1)
16
        y_dev = np.expand_dims(np.asarray(labels["devY"]), axis = 1)
17
        y_test = np.expand_dims(np.asarray(labels["testY"]), axis = 1)
18
        # X Dev
19
        X_dev = np.zeros((len(labels["devX"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))
20
        for imagenum in range(len(labels["devX"])):
21
            im_arr = cv2.imread(path_to("dev", imagenum))
22
            X_dev[imagenum] = im_arr
23
        print("Finished reading dev set")
24
        # X Test
25
        X_test = np.zeros((len(labels["testX"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))
26
        for imagenum in range(len(labels["testX"])):
27
            im_arr = cv2.imread(path_to("test", imagenum))
28
            X_test[imagenum] = im_arr
29
        print("Finished reading test set")
30
        # X Train
31
        X_train = np.zeros((len(labels["trainX"]), IMAGE_HEIGHT, IMAGE_WIDTH, NUM_CHANNELS))
32
        for imagenum in range(len(labels["trainX"])):
33
            im_arr = cv2.imread(path_to("train", imagenum))
34
            X_train[imagenum] = im_arr
35
        print("Finished reading train set")
36
    return (X_train, y_train), (X_dev, y_dev), (X_test, y_test)
37
​
38
(X_train, y_train), (X_dev, y_dev), (X_test, y_test) = load_data()
Now, we are ready to train the neural network with out data

The next segment contains the code that actually runs (cifar10_fractal.py)


10
1
import os
2
import glob
3
import argparse
4
from keras.callbacks import (
5
    LearningRateScheduler,
6
    ModelCheckpoint
7
)
8
from keras.datasets import cifar10
9
from keras.layers import (
10
    Activation,
11
    Input,
12
    Dense,
13
    Flatten
14
)
15
from keras.models import Model
16
from keras.optimizers import SGD, RMSprop, Adam, Nadam
17
#from keras.utils.visualize_util import plot
18
from keras.utils import np_utils
19
from keras import backend as K
20
​
21
​
22
NB_CLASSES = 10 #Original value is 10
23
NB_EPOCHS = 4 #Original value is 400
24
LEARN_START = 0.02
25
BATCH_SIZE = 100
26
MOMENTUM = 0.9
27
IMG_SIZE = 32
28
​
29
Y_train = np_utils.to_categorical(y_train, NB_CLASSES)
30
Y_dev = np_utils.to_categorical(y_dev, NB_CLASSES)
31
Y_test = np_utils.to_categorical(y_test, NB_CLASSES)
32
​
33
X_train = X_train.astype('float32')
34
X_dev = X_dev.astype('float32')
35
X_test = X_test.astype('float32')
36
​
37
X_train /= 255
38
X_dev /= 255
39
X_test /= 255
40
​
41
# Drop by 10 when we halve the number of remaining epochs (200, 300, 350, 375)
42
def learning_rate(epoch):
43
    if epoch < 200:
44
        return 0.02
45
    if epoch < 300:
46
        return 0.002
47
    if epoch < 350:
48
        return 0.0002
49
    if epoch < 375:
50
        return 0.00002
51
    return 0.000002
52
​
53
def build_network(deepest=False):
54
    dropout = [0., 0.1, 0.2, 0.3, 0.4]
55
    conv = [(64, 3, 3), (128, 3, 3), (256, 3, 3), (512, 3, 3), (512, 2, 2)]
56
    input= Input(shape=(3, IMG_SIZE, IMG_SIZE) if K._BACKEND == 'theano' else (IMG_SIZE, IMG_SIZE,3))
57
    output = fractal_net(
58
        c=3, b=5, conv=conv,
59
        drop_path=0.15, dropout=dropout,
60
        deepest=deepest)(input)
61
    output = Flatten()(output)
62
    output = Dense(NB_CLASSES, kernel_initializer='he_normal')(output)
63
    output = Activation('softmax')(output)
64
    model = Model(inputs=input, outputs=output)
65
    #optimizer = SGD(lr=LEARN_START, momentum=MOMENTUM)
66
    #optimizer = SGD(lr=LEARN_START, momentum=MOMENTUM, nesterov=True)
67
    optimizer = Adam()
68
    #optimizer = Nadam()
69
    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])
70
    #plot(model, to_file='model.png', show_shapes=True)
71
    return model
72
​
73
def train_network(net):
74
    print("Training network")
75
    snapshot = ModelCheckpoint(
76
        filepath="snapshots/weights.{epoch:04d}-{val_loss:.4f}.h5",
77
        monitor="val_loss",
78
        save_best_only=False)
79
    learn = LearningRateScheduler(learning_rate)
80
    net.fit(
81
        x=X_train, y=y_train, batch_size=BATCH_SIZE,
82
        epochs=NB_EPOCHS, validation_data=(X_test, y_test),
83
        #callbacks=[learn, snapshot]
84
        callbacks=[snapshot]
85
    )
86
​
87
def test_network(net, weights):
88
    print("Loading weights from '{}' and testing".format(weights))
89
    net.load_weights(weights)
90
    ret = net.evaluate(x=X_test, y=y_test, batch_size=BATCH_SIZE)
91
    print('Test:', ret)
92
​
93
def classify_image(net, weights, img):
94
    print("Loading weights from '{}' and testing".format(weights))
95
    net.load_weights(weights)
96
    ret = net.predict(img)
97
    print('Predictions: ', ret)
98
    
99
def main():
100
    # Since argparsers don't work in jupyter notebooks, enter all args manually here. 
101
    args = {"deepest": False, "load": None, "summary": False}
102
    classify = False;
103
    net = build_network(deepest=args["deepest"])
104
    if args["load"]:
105
        weights_filepath = "snapshots/weights.{epoch:04d}-{val_loss:.4f}.h5" #Fill this in with the location of the file containting the weights
106
        weights = weights_filepath
107
        test_network(net, weights)
108
    elif args["summary"]:
109
        net.summary()
110
#    elif classify:
111
#        image = "C:\\Users\\noahj\\cs230\\CS230 Project\\train_img\\img0.jpg"
112
#        image = np.expand_dims(cv2.imread(image, imagenum), axis=0)
113
#        classify_image(net, weights, image)
114
    else:
115
        train_network(net)
116
​
117
main()
